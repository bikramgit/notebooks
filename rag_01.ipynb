{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0bLqNlbp5psUIGSIb5VwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bikramgit/notebooks/blob/main/rag_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAtkqSti8DCy"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8J16FGNBLYE",
        "outputId": "e256358c-ac33-469d-ab92-7827b52e8140"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "id": "Wb7WGGk9BxpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj2Fsm8CCRPA",
        "outputId": "883df0f6-37b1-40ca-82e8-6858f9501840"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Google Gemini: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "id": "Xi5Iq4zYCl1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
      ],
      "metadata": {
        "id": "51lyiFvVCtjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-core"
      ],
      "metadata": {
        "id": "MxYvqz52C11T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "jBQW9Jk3C8qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c40140cc"
      },
      "source": [
        "%pip install -qU langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb8d3bb9",
        "outputId": "393aeff5-8eaa-4cec-cd93-6872eb9e3393"
      },
      "source": [
        "%pip install -qU langchain-hub\n",
        "%pip install -U langchain-core langchain-prompts\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-hub (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-hub\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-prompts (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-prompts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchainhub import Client\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# ðŸ§  Assume you have an initialized vector store and LLM here\n",
        "# For example:\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "# vector_store = FAISS.from_documents(all_splits, OpenAIEmbeddings())\n",
        "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# âœ… Use the new LangChainHub client\n",
        "hub = Client()\n",
        "prompt_text = hub.pull(\"rlm/rag-prompt\")\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the question below:\n",
        "ID: {id}\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "\"\"\")\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content, \"id\": \"001\"})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koc3KsIgDGzQ",
        "outputId": "466b78ce-ccac-467b-e784-9d9209106b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1310301242.py:33: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
            "Please use the `langsmith sdk` instead:\n",
            "  pip install langsmith\n",
            "Use the `pull_prompt` method.\n",
            "  prompt_text = hub.pull(\"rlm/rag-prompt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.input_variables)\n",
        "# #response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "# messages = prompt.invoke({\n",
        "#     '\"id\"': \"001\",  # notice quotes inside key\n",
        "#     \"question\": state[\"question\": \"What is Task Decomposition?\"],\n",
        "#     \"context\": docs_content\n",
        "# })\n",
        "# print(messages[\"answer\"])\n",
        "\n",
        "# # Prepare your question and context manually\n",
        "# question = \"What is Task Decomposition?\"\n",
        "# docs_content = \"\\n\\n\".join(doc.page_content for doc in all_splits[:3])  # sample chunks\n",
        "\n",
        "# # Invoke the prompt correctly\n",
        "# messages = prompt.invoke({\n",
        "#     \"id\": \"001\",          # satisfies weird template variable\n",
        "#     \"question\": question,\n",
        "#     \"context\": docs_content\n",
        "# })\n",
        "\n",
        "# # Show the generated prompt text\n",
        "# print(messages)\n",
        "# print(prompt.input_variables)\n",
        "# response = graph.invoke({\"id\": \"001\", \"question\": \"What is Task Decomposition?\"})\n",
        "# print(response[\"answer\"])\n",
        "\n",
        "docs_content = \"\\n\\n\".join(doc.page_content for doc in all_splits[:3])  # sample chunks\n",
        "\n",
        "# Call graph.invoke with all required variables\n",
        "response = graph.invoke({\n",
        "    \"id\": \"001\",                    # required by prompt\n",
        "    \"question\": \"What is Task Decomposition?\",\n",
        "    \"context\": docs_content         # the retrieved context for RAG\n",
        "})\n",
        "\n",
        "# Print the answer\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1eSRgXSFq8P",
        "outputId": "bfcf203f-4b29-4d7f-9f74-cfd0d05c8ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['context', 'id', 'question']\n",
            "Task Decomposition is the process of breaking down a large, complex task into smaller, more manageable, and often interdependent sub-tasks or components.\n",
            "\n",
            "**Key aspects:**\n",
            "*   **Hierarchical Structure:** It typically involves creating a hierarchical structure, where a main task is broken into several sub-tasks, and those sub-tasks might be further broken down into even smaller units, until each unit is specific, actionable, and easy to understand.\n",
            "*   **Reduced Complexity:** The primary goal is to simplify a daunting or ambiguous task into distinct, well-defined pieces, making it easier to comprehend, plan, and execute.\n",
            "\n",
            "**Benefits of Task Decomposition:**\n",
            "*   **Improved Understanding:** Makes complex problems easier to grasp.\n",
            "*   **Better Planning & Estimation:** Allows for more accurate time, resource, and budget estimations for each sub-task.\n",
            "*   **Easier Management:** Facilitates the assignment of specific sub-tasks to individuals or teams.\n",
            "*   **Enhanced Tracking:** Progress can be monitored more effectively on a granular level.\n",
            "*   **Increased Focus:** Teams or individuals can concentrate on one manageable piece at a time.\n",
            "*   **Identification of Dependencies:** Helps in uncovering relationships and dependencies between different parts of the work.\n",
            "\n",
            "This technique is widely used in project management (e.g., Work Breakdown Structure), software development, problem-solving, and any field where complex objectives need to be achieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vPOzcbTMVnC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}